{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rQvDYEAij_an",
    "outputId": "e400d6cc-ffea-4f25-bfe0-228caea3e8a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet Gradio langchain langchain-community langchainhub langchain-chroma beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_G8BMgPCmJWx",
    "outputId": "f309c6d1-fd30-4ec3-8894-c86c70a6fe7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.4 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "co9FdkzhknOc",
    "outputId": "822c380d-99b5-4db5-a7cc-b5cd79de228a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-google-generativeai (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-google-generativeai\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Cell 1: install all required packages (using ASCII hyphens!)\n",
    "!pip install --quiet \\\n",
    "    langchain \\\n",
    "    langchain-community \\\n",
    "    langchainhub \\\n",
    "    langchain-chroma \\\n",
    "    beautifulsoup4 \\\n",
    "    google-generativeai \\\n",
    "    langchain-google-generativeai \\\n",
    "    chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeJrtCfBkHrf",
    "outputId": "ca538578-48b8-4b84-9c53-0dde6bcbb2d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful. Ready to build your Gemini RAG system.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and authentication for Gemini (Google Generative AI)\n",
    "\n",
    "import os\n",
    "\n",
    "# 1) Set your Google Cloud API key for Gemini\n",
    "#    Replace the string below with your actual key.\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBel0AIgjAxhS2p1dX0ihgFTfy1E23S0Jc\"\n",
    "\n",
    "# 2) LangChain core & Retriever components\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 3) Gemini‑specific embeddings & chat\n",
    "from langchain_google_genai import (\n",
    "    GoogleGenerativeAIEmbeddings,\n",
    "    ChatGoogleGenerativeAI\n",
    ")\n",
    "\n",
    "# 4) Document loading and splitting\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# 5) Verify imports (optional sanity check)\n",
    "print(\"✅ Imports successful. Ready to build your Gemini RAG system.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bWIaHkqfkJ30"
   },
   "outputs": [],
   "source": [
    "  import warnings\n",
    "  warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3zzTZ-RmPpj",
    "outputId": "dfde633b-b819-4f17-8a6f-37010916f04f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Loaded 200 abstracts from arXiv.\n",
      "✅ Indexed 463 chunks into Chroma.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Fetch arXiv abstracts via API, split, embed & index\n",
    "\n",
    "# 0) Install & import arxiv\n",
    "!pip install --quiet arxiv\n",
    "import arxiv\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 1) Helper to load real abstracts from arXiv\n",
    "def load_arxiv_docs(query: str, max_results: int = 100):\n",
    "    results = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    docs = []\n",
    "    for r in results.results():\n",
    "        docs.append(Document(\n",
    "            page_content=r.summary,\n",
    "            metadata={\n",
    "                \"source\": r.entry_id,      # e.g. 'http://arxiv.org/abs/2101.00001'\n",
    "                \"title\":  r.title\n",
    "            }\n",
    "        ))\n",
    "    return docs\n",
    "\n",
    "# 2) Build your corpus\n",
    "# Fetch 200 abstracts across multiple categories for broad coverage:\n",
    "cats = [\"cs.AI\", \"cs.CV\", \"cs.CL\", \"q-bio.GN\"]  # AI, Vision, NLP, Genomics\n",
    "raw_docs = []\n",
    "for cat in cats:\n",
    "    raw_docs += load_arxiv_docs(f\"cat:{cat}\", max_results=50)\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} abstracts from arXiv.\")\n",
    "\n",
    "# 3) Chunk them (so each fits in context)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "# 4) Embed & index in Chroma\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"chromadb_arxiv\"\n",
    ")\n",
    "vectordb.persist()\n",
    "\n",
    "print(f\" Indexed {len(docs)} chunks into Chroma.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcdzUT-6m84j",
    "outputId": "c9148858-78e3-4acb-ae82-d06de64ac312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Evolutionarily conserved human targets of adenosine to inosi… → identified and experimentally verified four additional candidate human\n",
      "substrates for ADAR-mediated editing: FLNA, BLCAP…\n",
      "[2] Systematic identification of abundant A-to-I editing sites i… → in Alu repeats. Analysis of the large set of editing sites indicates the role\n",
      "of editing in controlling dsRNA stability.…\n",
      "[3] Systematic identification of abundant A-to-I editing sites i… → search for ADAR editing sites in the human transcriptome, using millions of\n",
      "available expressed sequences. 12,723 A-to-I…\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Retrieval Function (unchanged)\n",
    "\n",
    "def retrieve(query: str, k: int = 5):\n",
    "    return vectordb.similarity_search(query=query, k=k)\n",
    "\n",
    "# Sanity check: try “CRISPR” now\n",
    "hits = retrieve(\"CRISPR-Cas9 gene editing\", k=3)\n",
    "for i, doc in enumerate(hits, 1):\n",
    "    print(f\"[{i}] {doc.metadata['title'][:60]}… → {doc.page_content[:120]}…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xO7sysV0n0hR",
    "outputId": "5b9521b6-ee2c-4cbf-bcbd-9a4321cc7495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RetrievalQA chain is ready with Gemini.\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# Cell 6: Setup Gemini Chat Model & RetrievalQA Chain\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 1) Instantiate the Gemini chat model with a valid model name\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",            # <— corrected\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# 2) Prompt template enforcing citations\n",
    "prompt_template = \"\"\"\n",
    "You are a research assistant. Use the context below to answer the question.\n",
    "Cite each fact by referring to the chunk number in square brackets (e.g. [1], [2]).\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# 3) Build the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "print(\" RetrievalQA chain is ready with Gemini.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxVuQchbn5er",
    "outputId": "70f74309-6559-4b59-fadc-dba52c21a85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " The provided text does not define or mention federated learning.  It discusses a general learning algorithm applicable to problems with low-level membership values combined through an and-or tree structure [1]. It also discusses the interdependence of learning and reasoning in AI [2] and multi-agent reinforcement learning in the context of load balancing [3].  None of these concepts are described as or equivalent to federated learning. \n",
      "\n",
      "Sources:\n",
      "[1] http://arxiv.org/abs/cs/9510103v1 → labeled with their desired category measure. The learning algorithm is\n",
      "generally applicable to any p…\n",
      "[2] http://arxiv.org/abs/cs/9508102v1 → Learning and reasoning are both aspects of what is considered to be\n",
      "intelligence. Their studies with…\n",
      "[3] http://arxiv.org/abs/cs/9505102v1 → We study the process of multi-agent reinforcement learning in the context of\n",
      "load balancing in a dis…\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# Cell 7: answer_query() helper\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def answer_query(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Runs the RAG chain and returns (answer, source_documents).\n",
    "    \"\"\"\n",
    "    qa_chain.retriever.search_kwargs[\"k\"] = k\n",
    "    result = qa_chain({\"query\": query})\n",
    "    answer = result[\"result\"]\n",
    "    sources = result[\"source_documents\"]  # List[Document]\n",
    "    return answer, sources\n",
    "\n",
    "# Quick test\n",
    "answer, sources = answer_query(\"What is federated learning?\", k=3)\n",
    "print(\"Answer:\\n\", answer, \"\\n\\nSources:\")\n",
    "for i, doc in enumerate(sources, start=1):\n",
    "    src = doc.metadata.get(\"source\", \"<no-source>\")\n",
    "    print(f\"[{i}] {src} → {doc.page_content[:100]}…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "74j1M5_in9ti",
    "outputId": "99ee920b-8906-44bc-ae4a-e634b2b8fa74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://69f8b230b18a8dd3c9.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://69f8b230b18a8dd3c9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# Cell 8: Gradio UI (Interactive Chat + Clickable Sources)\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def ui_fn(query, history, k):\n",
    "    history = history + [{\"role\": \"user\", \"content\": query}]\n",
    "    answer, docs = answer_query(query, k)\n",
    "    history = history + [{\"role\": \"assistant\", \"content\": answer}]\n",
    "\n",
    "    # Build clickable source list\n",
    "    sources_html = \"<ul>\"\n",
    "    for i, doc in enumerate(docs, start=1):\n",
    "        src = doc.metadata.get(\"source\", \"#\")\n",
    "        sources_html += f\"<li><a href='{src}' target='_blank'>[{i}] {src}</a></li>\"\n",
    "    sources_html += \"</ul>\"\n",
    "\n",
    "    return history, \"\", sources_html\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"##  Gemini‑Powered RAG Q&A\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(label=\"Conversation\", type=\"messages\")\n",
    "            user_input = gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"Type any research question…\",\n",
    "                lines=1\n",
    "            )\n",
    "            k_slider = gr.Slider(\n",
    "                minimum=1, maximum=5, step=1, value=3,\n",
    "                label=\"Number of sources to retrieve (k)\"\n",
    "            )\n",
    "            ask_btn = gr.Button(\" Ask\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### Retrieved Sources\")\n",
    "            source_output = gr.HTML()\n",
    "\n",
    "    ask_btn.click(\n",
    "        fn=ui_fn,\n",
    "        inputs=[user_input, chatbot, k_slider],\n",
    "        outputs=[chatbot, user_input, source_output]\n",
    "    )\n",
    "    user_input.submit(\n",
    "        fn=ui_fn,\n",
    "        inputs=[user_input, chatbot, k_slider],\n",
    "        outputs=[chatbot, user_input, source_output]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "najrDwR9pRmc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
